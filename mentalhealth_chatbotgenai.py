# -*- coding: utf-8 -*-
"""MentalHealth_ChatbotGENAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AgHPcsEfl8qVkBclYxQajFG3yUXhC4_d
"""

!pip install langchain_core langchain_community

!pip install langchain_groq

from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)

result = llm.invoke("Who is lord Ram?")
print(result)

!pip install pypdf

!pip install chromadb

pip install -U langchain-huggingface

pip install -U langchain-community

import os

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

def initialize_llm():
  llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
  return llm

def create_vector_db():
  loader = DirectoryLoader("/content/data", glob = '*.pdf', loader_cls = PyPDFLoader)
  documents = loader.load()
  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)
  texts = text_splitter.split_documents(documents)
  embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
  vector_db = Chroma.from_documents(texts, embeddings, persist_directory = './chroma_db')
  vector_db.persist()

  print("ChromaDB created and data saved")

  return vector_db

def setup_qa_chain(vector_db, llm):
  retriever = vector_db.as_retriever()
  prompt_templates = """ You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
    {context}
    User: {question}
    Chatbot: """
  PROMPT = PromptTemplate(template = prompt_templates, input_variables = ['context', 'question'])

  qa_chain = RetrievalQA.from_chain_type(
      llm = llm,
      chain_type = "stuff",
      retriever = retriever,
      chain_type_kwargs = {"prompt": PROMPT}
  )
  return qa_chain


def main():
  print("Intializing Chatbot.........")
  llm = initialize_llm()

  db_path = "/content/chroma_db"

  if not os.path.exists(db_path):
    vector_db  = create_vector_db()
  else:
    embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  qa_chain = setup_qa_chain(vector_db, llm)

  while True:
    query = input("\nHuman: ")
    if query.lower()  == "exit":
      print("Chatbot: Take Care of yourself, Goodbye!")
      break
    response = qa_chain.run(query)
    print(f"Chatbot: {response}")

if __name__ == "__main__":
  main()

pip install -U langchain-huggingface

from langchain_huggingface import HuggingFaceEmbeddings

!pip install gradio

pip install langchain-openai

from langchain_openai import ChatOpenAI

# Install required packages (uncomment if not already installed)
# !pip install langchain gradio chromadb sentence-transformers

!pip install langchain_groq # Install langchain_groq package

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq  # Import ChatGroq from langchain_groq
import gradio as gr
import os
def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama-3.3-70b-versatile"
    )
    return llm

def create_vector_db():
    # Load all PDF documents from /content/data/
    loader = DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
    vector_db.persist()
    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_templates = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
{context}
User: {question}
Chatbot:"""
    PROMPT = PromptTemplate(template=prompt_templates, input_variables=['context', 'question'])
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# Initialize chatbot components
print("Initializing Chatbot...")
llm = initialize_llm()

db_path = "/content/chroma_db"
if not os.path.exists(db_path):
    vector_db = create_vector_db()
else:
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

qa_chain = setup_qa_chain(vector_db, llm)

# Define the function that Gradio will use
def chatbot(query):
    if query.strip().lower() == "exit":
        return "Take Care of yourself, Goodbye!"
    response = qa_chain.run(query)
    return response

# Create and launch the Gradio Interface
iface = gr.Interface(
    fn=chatbot,
    inputs="text",
    outputs="text",
    title="Mental Health Chatbot",
    description="A compassionate mental health chatbot powered by LangChain."
)

iface.launch()

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
import gradio as gr
import os

def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama-3.3-70b-versatile"
    )
    return llm

def create_vector_db():
    loader = DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)

    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
    vector_db.persist()

    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()

    prompt_templates = """You are a compassionate mental health chatbot. Respond thoughtfully to the following conversation history and question:
    Conversation History: {context}
    User: {question}
    Chatbot:"""
    PROMPT = PromptTemplate(template=prompt_templates, input_variables=['context', 'question'])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

print("Initializing Chatbot...")
llm = initialize_llm()

db_path = "./chroma_db"

if not os.path.exists(db_path):
    print("Creating Vector Database...")
    vector_db = create_vector_db()
else:
    print("Loading Vector Database...")
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
    print("Vector Database loaded")

qa_chain = setup_qa_chain(vector_db, llm)

conversation_history = []  # It must be a list of lists, e.g., [["User message", "Bot response"]]

def chatbot_response(user_input):
    global conversation_history

    if not user_input.strip():
        return "Please provide valid input.", conversation_history

    if user_input.lower() == "exit":
        conversation_history = []
        return "Take Care of yourself, Goodbye!", []

    context = "\n".join([f"User: {x[0]}\nChatbot: {x[1]}" for x in conversation_history])

    try:
        # Pass only the query to qa_chain
        response = qa_chain.run(user_input)
        response_text = response if isinstance(response, str) else str(response)

        # Append the conversation properly as a list of two items
        conversation_history.append([user_input, response_text])

        return "", conversation_history  # Return the updated conversation history
    except Exception as e:
        return f"An error occurred: {str(e)}", conversation_history


# Creating Gradio Interface with gr.Blocks()
with gr.Blocks() as demo:
    chatbot = gr.Chatbot(label="Mental Health Chatbot")
    msg = gr.Textbox(placeholder="Enter your message...", label="User Input")
    clear = gr.Button("Clear Chat")

    def clear_chat():
        global conversation_history
        conversation_history = []
        return conversation_history  # Return empty conversation history for clearing

    msg.submit(chatbot_response, inputs=msg, outputs=[msg, chatbot], queue=False)
    clear.click(clear_chat, outputs=chatbot)

demo.launch(share=True)

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
import gradio as gr
import os

def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama-3.3-70b-versatile"
    )
    return llm

def create_vector_db():
    loader = DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)

    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
    vector_db.persist()

    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()

    prompt_templates = """You are a compassionate mental health chatbot. Respond thoughtfully to the following conversation history and question:
    Conversation History: {context}
    User: {question}
    Chatbot:"""
    PROMPT = PromptTemplate(template=prompt_templates, input_variables=['context', 'question'])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

print("Initializing Chatbot...")
llm = initialize_llm()

db_path = "./chroma_db"

if not os.path.exists(db_path):
    print("Creating Vector Database...")
    vector_db = create_vector_db()
else:
    print("Loading Vector Database...")
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
    print("Vector Database loaded")

qa_chain = setup_qa_chain(vector_db, llm)

conversation_history = []

def chatbot_response(user_input):
    global conversation_history

    if not user_input.strip():
        return "Please provide valid input.", conversation_history

    if user_input.lower() == "exit":
        conversation_history = []
        return "Take Care of yourself, Goodbye!", []

    context = "\n".join([f"User: {x[0]}\nChatbot: {x[1]}" for x in conversation_history])

    try:
        response = qa_chain.run(user_input)
        response_text = response if isinstance(response, str) else str(response)

        conversation_history.append([user_input, response_text])

        return "", conversation_history
    except Exception as e:
        return f"An error occurred: {str(e)}", conversation_history

def clear_chat():
    global conversation_history
    conversation_history = []
    return conversation_history

# Creating Gradio Interface with Better UI
with gr.Blocks(theme='soft') as demo:
    gr.Markdown(
        """
        <style>
        body {background-color: #f0f0f0;}
        .gradio-container {max-width: 800px; margin: auto; padding: 20px; border-radius: 20px; background-color: white; box-shadow: 0 4px 12px rgba(0,0,0,0.1);}
        .chatbot-message {background-color: #e0f7fa; padding: 10px; border-radius: 10px; margin: 5px;}
        .user-message {background-color: #fff3e0; padding: 10px; border-radius: 10px; margin: 5px;}
        </style>
        """
    )

    gr.Markdown("# ðŸŒ¼ Compassionate Mental Health Chatbot")

    chatbot = gr.Chatbot(label="Chat with me ðŸ¤–")
    msg = gr.Textbox(placeholder="Enter your message...", label="Type something")
    clear = gr.Button("ðŸ§¹ Clear Chat")

    msg.submit(chatbot_response, inputs=msg, outputs=[msg, chatbot], queue=False)
    clear.click(clear_chat, outputs=chatbot)

demo.launch(share=True)

